{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed026c28",
      "metadata": {},
      "source": [
        "# RAG Pipeline for Q&A over a Text File\n",
        "\n",
        "This notebook implements a clean Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "1.  **Install** required libraries.\n",
        "2.  **Load** an `OPENAI_API_KEY` (if available).\n",
        "3.  **Load** a source `.txt` file.\n",
        "4.  **Chunk, Embed, & Store** the text in a Chroma vector database.\n",
        "5.  **Build** a LangChain RAG chain to answer questions.\n",
        "6.  **Run** an interactive chat loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bf1e322f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.10.19 (main, Oct 21 2025, 16:37:10) [Clang 20.1.8 ]\n"
          ]
        }
      ],
      "source": [
        "## 1) Install dependencies\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "# Core libs\n",
        "!pip -q install langchain langchain-community chromadb sentence-transformers\n",
        "\n",
        "# For optional local LLM fallback\n",
        "!pip -q install transformers accelerate\n",
        "\n",
        "# For OpenAI\n",
        "!pip -q install langchain-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "29c4f426",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## 2) Load API Key\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# *** UPDATE THIS PATH to your .env file ***\n",
        "env_path = Path(\"/Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/ATT81022.env\")\n",
        "load_dotenv(dotenv_path=env_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "027d7163",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Using OpenAI for generation.\n",
            "CHROMA_DIR = /Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/chroma\n",
            "COLLECTION  = uploaded_text\n"
          ]
        }
      ],
      "source": [
        "## 3) Set Constants & Check Key\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Path where Chroma (vector DB) will be persisted\n",
        "CHROMA_DIR = \"./chroma\"\n",
        "COLLECTION = \"uploaded_text\"\n",
        "\n",
        "# --- Optional: OpenAI ---\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n",
        "USE_OPENAI = bool(OPENAI_API_KEY)\n",
        "\n",
        "if USE_OPENAI:\n",
        "    print(\"‚úÖ Using OpenAI for generation.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è OPENAI_API_KEY not set ‚Äî will use local Transformers fallback.\")\n",
        "\n",
        "Path(CHROMA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "print(\"CHROMA_DIR =\", Path(CHROMA_DIR).resolve())\n",
        "print(\"COLLECTION  =\", COLLECTION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f46b1af8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 27,327 characters from: /Volumes/Untitled/Youtube_QA_Rag_System/Working_Pipelines/text/RAG_TEXT.txt\n"
          ]
        }
      ],
      "source": [
        "## 4) Load Text Document\n",
        "\n",
        "# *** UPDATE THIS PATH to your .txt file ***\n",
        "uploaded_path = \"/Volumes/Untitled/Youtube_QA_Rag_System/Working_Pipelines/text/RAG_TEXT.txt\"\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(uploaded_path).expanduser()\n",
        "assert p.exists(), f\"File not found: {p}\"\n",
        "\n",
        "text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "print(f\"Loaded {len(text):,} characters from:\", p.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "27a7dcdf",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using ChatOpenAI: gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "## 5) Define LLM (Generator)\n",
        "\n",
        "generator = None\n",
        "\n",
        "if USE_OPENAI:\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    generator = llm\n",
        "    print(\"Using ChatOpenAI: gpt-4o-mini\")\n",
        "else:\n",
        "    # Local Transformers text2text generation via HF pipeline\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "    print(\"Loading local model: google/flan-t5-base...\")\n",
        "    model_id = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "    hf_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    class HFText2TextLLM:\n",
        "        def __call__(self, prompt_text: str) -> str:\n",
        "            out = hf_pipe(prompt_text, max_new_tokens=256, truncation=True)\n",
        "            return out[0][\"generated_text\"]\n",
        "    \n",
        "    generator = HFText2TextLLM()\n",
        "    print(\"Using local Transformers: flan-t5-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "182a66aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-10 (bg_main):\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/59/_rmmbn7d6kx3y2yg0wqg8ghc0000gn/T/ipykernel_16260/310511275.py\", line 17, in bg_main\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/IPython/core/display_functions.py\", line 374, in update\n",
            "    update_display(obj, display_id=self.display_id, **kwargs)\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/IPython/core/display_functions.py\", line 326, in update_display\n",
            "    display(obj, display_id=display_id, **kwargs)\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/IPython/core/display_functions.py\", line 296, in display\n",
            "    publish_display_data(data=obj, metadata=metadata, **kwargs)\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/IPython/core/display_functions.py\", line 93, in publish_display_data\n",
            "    display_pub.publish(\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 135, in publish\n",
            "    msg = self.session.msg(msg_type, json_clean(content), parent=self.parent_header)\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 69, in parent_header\n",
            "    return self._parent_header.get()\n",
            "LookupError: <ContextVar name='parent_header' at 0x103ff7ec0>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 788, in run_closure\n",
            "    _threading_Thread_run(self)\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/var/folders/59/_rmmbn7d6kx3y2yg0wqg8ghc0000gn/T/ipykernel_16260/310511275.py\", line 19, in bg_main\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/IPython/core/display_functions.py\", line 374, in update\n",
            "    update_display(obj, display_id=self.display_id, **kwargs)\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/IPython/core/display_functions.py\", line 326, in update_display\n",
            "    display(obj, display_id=display_id, **kwargs)\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/IPython/core/display_functions.py\", line 296, in display\n",
            "    publish_display_data(data=obj, metadata=metadata, **kwargs)\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/IPython/core/display_functions.py\", line 93, in publish_display_data\n",
            "    display_pub.publish(\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 135, in publish\n",
            "    msg = self.session.msg(msg_type, json_clean(content), parent=self.parent_header)\n",
            "  File \"/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 69, in parent_header\n",
            "    return self._parent_header.get()\n",
            "LookupError: <ContextVar name='parent_header' at 0x103ff7ec0>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks created: 116\n",
            "‚úÖ Stored in Chroma at: /Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/chroma\n",
            "\n",
            "‚úÖ Created 'retriever' variable.\n"
          ]
        }
      ],
      "source": [
        "## 6) Chunk, Embed, and Store in Vector DB\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "#info you can create a fuction for the below chucking for code clarity it would easier to read the codes and understand for\n",
        "# 1) Chunk the text \n",
        "# 2) create text to speech into my code\n",
        "# 3) \n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        ")\n",
        "docs = [Document(page_content=c, metadata={\"source\": str(p.name)}) \n",
        "        for c in splitter.split_text(text)]\n",
        "print(f\"Chunks created: {len(docs)}\")\n",
        "\n",
        "# 2) Embedding function\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# 3) Create (or re-open) the Chroma collection\n",
        "vs = Chroma(\n",
        "    collection_name=COLLECTION,\n",
        "    persist_directory=CHROMA_DIR,\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "# 4) Add docs\n",
        "vs.add_documents(docs)\n",
        "print(\"‚úÖ Stored in Chroma at:\", Path(CHROMA_DIR).resolve())\n",
        "\n",
        "# 5) Create the retriever\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"\\n‚úÖ Created 'retriever' variable.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c46af5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Multi-Query Retriever active.\n"
          ]
        }
      ],
      "source": [
        "## 6.5.1 Multi-Query Retriever\n",
        "\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "# We are using OpenAI, \n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=vs.as_retriever(search_kwargs={\"k\": 5}),\n",
        "    llm=llm\n",
        ")\n",
        "print(\"‚úÖ Multi-Query Retriever active.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45af9258",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Multi-Query Retriever (OpenAI) is active.\n",
            "   (The system will now generate variations of users questions for better search results.)\n"
          ]
        }
      ],
      "source": [
        "## 6.5)(Not be used in my code) Depeciated - Upgrade to Multi-Query Retriever - D\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "import logging\n",
        "\n",
        "# Optional: Turn on logging so you can see the different questions the AI generates\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "if USE_OPENAI:\n",
        "    # This uses the LLM (gpt-4o-mini) to generate variations of the question\n",
        "    # and retrieve documents for all variations.\n",
        "    retriever = MultiQueryRetriever.from_llm(\n",
        "        retriever=vs.as_retriever(search_kwargs={\"k\": 9}),\n",
        "        llm=llm\n",
        "    )\n",
        "    print(\"‚úÖ Multi-Query Retriever (OpenAI) is active.\")\n",
        "    print(\"   (The system will now generate variations of users questions for better search results.)\")\n",
        "\n",
        "else:\n",
        "    # Fallback: Multi-query requires a strong instruction-following LLM.\n",
        "    # Smaller local models (like flan-t5) often fail the strict formatting requirements.\n",
        "    retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "    print(\"‚ÑπÔ∏è Using standard retriever (Local Model).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "af27e405",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Conversational RAG (OpenAI + Memory) is ready.\n"
          ]
        }
      ],
      "source": [
        "## 7) Build Conversational RAG Chain (With Memory)\n",
        "\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "\n",
        "\n",
        "# --- 1. Contextualize Question ---\n",
        "# This prompt helps the LLM understand follow-up questions (e.g., \"What about him?\")\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history.Tell user politly the the history does not exit, \"\n",
        "    \n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", contextualize_q_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# --- 2. Answer Question ---\n",
        "# This is the prompt that actually answers the user\n",
        "qa_system_prompt = (\n",
        "    \"Your name is Chiity \"\n",
        "    \"You are a here to help discussed the tools *Give summary of the tools in the text*. **Be concise.** \"\n",
        "    \"Do not call it text, say it is a knowlegde base\"\n",
        "     \"Give the initial summary of what you here for \"\n",
        "    \"Do not tell anyone you were trainned by Open AI\"\n",
        "    \"You are a helpful assistant. Answer the question only from the provided context. \"\n",
        "    \"You may engage in friendly conversation, but never fabricate facts outside the context. \"\n",
        "    \"Analyze the text and always give good sumaries and action point available\"\n",
        "    \"You answer should be concise\"\n",
        "    \"Limite the use of tokens and be very concise\"\n",
        "\n",
        "\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", qa_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# --- 3. Build the Chain ---\n",
        "if USE_OPENAI:\n",
        "    # Create a retriever that can handle history\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm, retriever, contextualize_q_prompt\n",
        "    )\n",
        "    \n",
        "    # Create the document combining chain\n",
        "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "    \n",
        "    # Combine them\n",
        "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "    # --- 4. Memory Management ---\n",
        "    store = {}\n",
        "\n",
        "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "        if session_id not in store:\n",
        "            store[session_id] = ChatMessageHistory()\n",
        "        return store[session_id]\n",
        "\n",
        "    conversational_chain = RunnableWithMessageHistory(\n",
        "        rag_chain,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "        output_messages_key=\"answer\",\n",
        "    )\n",
        "    print(\"‚úÖ Conversational RAG (OpenAI + Memory) is ready.\")\n",
        "\n",
        "else:\n",
        "    # Local Fallback (Simplified memory for local testing)\n",
        "    # Note: Local models often struggle with the complex history-rewriting step\n",
        "    print(\"‚ÑπÔ∏è Memory is disabled for local fallback to ensure stability.\")\n",
        "    conversational_chain = None \n",
        "    # We will handle the fallback logic in the ask function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1c0a9016",
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7.5) Simple Hallucination Check (Cosine Similarity)\n",
        "\n",
        "# 0.0 = No match, 1.0 = Perfect match. \n",
        "# 0.7 is a good starting threshold for \"all-MiniLM-L6-v2\".\n",
        "SIMILARITY_THRESHOLD = 0.2\n",
        "\n",
        "def get_relevant_context_only(question: str):\n",
        "    \"\"\"\n",
        "    Retrieves docs but stops if they aren't similar enough to the question.\n",
        "    Returns: (is_safe: bool, context_text: str)\n",
        "    \"\"\"\n",
        "    # Get docs with their scores\n",
        "    results = vs.similarity_search_with_relevance_scores(question, k=3)\n",
        "    \n",
        "    if not results:\n",
        "        return False, \"No documents found.\"\n",
        "        \n",
        "    # Check the top score\n",
        "    top_doc, top_score = results[0]\n",
        "    \n",
        "    print(f\"üîç Interaction Check: Top Score = {top_score:.4f}\")\n",
        "    \n",
        "    if top_score < SIMILARITY_THRESHOLD:\n",
        "        print(\"‚ö†Ô∏è Score too low - Preventing Hallucination.\")\n",
        "        return False, None\n",
        "        \n",
        "    # If safe, format the docs into a string\n",
        "    context_text = \"\\n\\n\".join([f\"{doc.page_content}\" for doc, _ in results])\n",
        "    return True, context_text\n",
        "\n",
        "# --- Updated Ask Function ---\n",
        "def ask_safe(question: str):\n",
        "    is_safe, context = get_relevant_context_only(question)\n",
        "    \n",
        "    if not is_safe:\n",
        "        return \"I'm sorry, I don't have enough information in the uploaded text to answer that confidently.\"\n",
        "    \n",
        "    # If safe, proceed with the standard LLM generation\n",
        "    # We inject the verified context manually\n",
        "    prompt_text = (\n",
        "        f\"System: You are a helpful assistant. Use the context below.\\n\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "    \n",
        "    if USE_OPENAI:\n",
        "        from langchain_core.messages import HumanMessage\n",
        "        return llm.invoke([HumanMessage(content=prompt_text)]).content\n",
        "    else:\n",
        "        return generator(prompt_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b23737",
      "metadata": {},
      "source": [
        "## 8) Ask Questions\n",
        "\n",
        "Run the cells below to interact with your RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "46401dbb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† Chatbot with Memory is ready (Session ID: user_session_1).\n",
            "Try asking a question, then a follow-up like 'Telis l me more about that'.\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Hi there, how can I assist you today?  ', '2. What information or help are you looking for?  ', '3. Can you tell me what you need assistance with?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today? If you have questions about the tools in the knowledge base, feel free to ask!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: [\"1. What assistance can I provide you with today, even though I don't have a personal name?  \", '2. How may I assist you today, considering I go by the name Assistant?  ', '3. In what ways can I help you today, despite not having a personal name?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My name is Chiity. I'm here to help you with information about the tools in the knowledge base. How can I assist you?\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: [\"1. Can you clarify what specific information or topic you need assistance with, since I don't have access to a chat history or summary?\", \"2. What particular subject or question do you have in mind that I can help you with, given that I don't have access to previous conversations?\", \"3. Since I can't access past chat histories or summaries, what specific details or topics are you interested in discussing?\"]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The knowledge base provides information on various tools within the Content Hub Media Production Suite, specifically focusing on VFX Pulls and Footage Ingest. Key tools include:\n",
            "\n",
            "1. **VFX Pulls**: Facilitates color and framing management for visual effects, allowing post-production teams to manage media efficiently.\n",
            "2. **ConformPolls**: Automates the request for original camera files, reducing delays.\n",
            "3. **Footage Ingest**: Scans directories for errors and organizes media formats before uploads, enabling tracking of progress from anywhere.\n",
            "\n",
            "These tools enhance collaboration and streamline the media management process for creative teams. If you have any specific questions about these tools, let me know!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: [\"1. Can you provide your name or any other details you'd like to share? I'm here to help with any questions you may have!\", \"2. If you're comfortable, could you tell me your name or ask any other questions you might have?\", \"3. Would you like to share your name or discuss something else? I'm available to assist with any inquiries!\"]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but I don't have that information. How can I assist you further?\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Hello Lukman! What can I do for you today?  ', '2. Hi Lukman! How may I help you at this moment?  ', '3. Greetings Lukman! What assistance do you need from me today?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nice to meet you, Lukman! How can I assist you today?\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How does Conform Pulls automate the request process for original camera files in post-production workflows?', '2. What features does Conform Pulls offer to enhance efficiency in obtaining camera files for visual effects and editing teams?', '3. Can you explain the benefits of using Conform Pulls for streamlining file requests in post-production projects?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ConformPulls is a tool within the Content Hub Media Production Suite designed to streamline the conform process for media production teams. Key features include:\n",
            "\n",
            "- **Automated Media Management**: It allows post teams to request original camera files effortlessly, reducing delays caused by human availability.\n",
            "- **Framing Management**: It helps customize framing for renders and delivers updated framing instructions for downstream use.\n",
            "- **Seamless Workflow**: Once an EDL is submitted, all media is automatically located, trimmed, and consolidated, ready for finishing.\n",
            "- **Look File Management**: Provides directories with look files for each plate delivered, ensuring accurate color management.\n",
            "\n",
            "Overall, ConformPulls enhances efficiency and reduces errors in the media production process. If you have more questions, feel free to ask!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What are the main functionalities offered by ConformPulls in media management?', 'Can you provide a summary of the key features and benefits of using ConformPulls for media requests and workflow?', 'How does ConformPulls enhance the efficiency of media handling and color management in production?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sure! Here‚Äôs a concise list of ConformPulls features:\n",
            "\n",
            "1. Automated media requests for original camera files.\n",
            "2. Reduces delays in the conform process.\n",
            "3. Customizable framing management for renders.\n",
            "4. Automatic media location, trimming, and consolidation after EDL submission.\n",
            "5. Provides look files for accurate color management.\n",
            "\n",
            "Let me know if you need more information!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How many questions have I asked up to this point, and can I ask more if needed?  ', \"2. I've already submitted six questions; what should I do if I have additional inquiries?  \", \"3. Can you tell me how many questions I've asked so far and if I can continue asking more?\"]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have asked a total of five questions so far. If you have more questions or need assistance, feel free to ask!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What additional support or information can I provide you with today, Lukman?  ', '2. How else can I help you today, Lukman?  ', '3. Is there anything specific you would like assistance with today, Lukman?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your name is Lukman. How can I assist you further?\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: [\"1. I don't have access to previous chat logs, but I'm here to assist you with any inquiries or subjects you want to explore!\", \"2. Unfortunately, I can't view past conversations, but feel free to ask me anything or bring up topics you're interested in!\", \"3. I can't access the chat history, but I'm ready to help you with any questions or discussions you have in mind!\"]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your last three messages were:\n",
            "\n",
            "1. \"give me a concise list.\"\n",
            "2. \"how many questions did i ask.\"\n",
            "3. \"what is my name.\"\n",
            "\n",
            "If you have more questions or need assistance, just let me know!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What are the main functionalities of the Footage Ingest tool in the Content Hub Media Production Suite?', 'Can you explain how Footage Ingest improves the media file management process for production teams?', 'What features does the Footage Ingest tool offer to enhance the efficiency of uploading and organizing media files?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Footage Ingest is a tool within the Content Hub Media Production Suite designed to streamline the integration of media into the Netflix ecosystem. Key features include:\n",
            "\n",
            "1. **Error Scanning**: Scans directories for errors and identifies media formats before uploads.\n",
            "2. **Automatic Organization**: Organizes content automatically based on identified media formats.\n",
            "3. **Progress Tracking**: Allows users to track upload progress from anywhere using Content Hub.\n",
            "4. **Customizable Workflows**: Tailors framing and color workflows to specific show needs.\n",
            "5. **Efficient Media Management**: Facilitates seamless uploads and prepares media for post-production.\n",
            "\n",
            "This tool enhances the efficiency of managing media from production to post-production. Let me know if you need more details!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How does Footage Ingest manage the process of uploading and preparing media for system integration, including error scanning and content organization?', '2. Can you explain the features of Footage Ingest related to media uploads, such as error detection, content organization, and upload progress tracking?', '3. What are the key functionalities of Footage Ingest when it comes to handling media uploads and ensuring they are ready for integration into the system?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, the Footage Ingest tool handles uploads by allowing users to upload original camera files and EDLs (Edit Decision Lists) directly into the system. It automates the process of locating, trimming, and consolidating media for post-production. If you have more questions, feel free to ask!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What are the key features and functionalities of Footage Ingest?', 'Can you provide a brief overview of how Footage Ingest manages media uploads and organization?', 'What processes does Footage Ingest use to ensure efficient media handling and error checking during uploads?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Apologies for that! Here‚Äôs a more concise summary of Footage Ingest:\n",
            "\n",
            "- **Handles uploads** of original camera files and EDLs.\n",
            "- **Scans for errors** and organizes media formats.\n",
            "- **Tracks progress** from anywhere via Content Hub.\n",
            "- **Prepares media** for post-production efficiently.\n",
            "\n",
            "Let me know if you need anything else!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key components and functions of Edit Decision Lists (EDLs) in video editing projects?', '2. How do Edit Decision Lists (EDLs) assist in the post-production process of video editing?', '3. Can you explain the role of EDLs in organizing and managing video clips during the editing workflow?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EDLs, or Edit Decision Lists, are files that contain a list of edits made to a video project, including information about which clips to use, their order, and timing. They facilitate the editing and conforming process in post-production.\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the purpose of an Edit Decision List (EDL) in video editing, and what information does it typically contain?', 'Can you explain how an EDL is utilized in the post-production process of video projects?', 'What details are included in an Edit Decision List, and how does it assist editors in assembling a video?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An EDL, or Edit Decision List, is a file that outlines the sequence of edits in a video project. It specifies which clips to use, their order, and timing, serving as a guide for editors during the post-production process.\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What additional support or information can I provide for you, Lukman?  ', '2. How else can I help you today, Lukman?  ', '3. Is there anything specific you would like assistance with, Lukman?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your name is Lukman. How can I assist you further?\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide more information or assistance regarding your name, Lukman?  ', '2. What additional questions do you have, Lukman, that I can help you with?  ', '3. If you need support or have inquiries, Lukman, what would you like to know?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your name is Lukman. If you have more questions or need assistance, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "## 8 & 9) Chat with Memory\n",
        "\n",
        "# Create a random session ID for this run\n",
        "session_id = \"user_session_1\"\n",
        "\n",
        "def ask_with_memory(question: str):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a non-empty question.\"\n",
        "    \n",
        "    if USE_OPENAI:\n",
        "        # Invoke with session_id so it remembers previous turns\n",
        "        response = conversational_chain.invoke(\n",
        "            {\"input\": question},\n",
        "            config={\"configurable\": {\"session_id\": session_id}}\n",
        "        )\n",
        "        return response[\"answer\"]\n",
        "    else:\n",
        "        # Fallback for local model (Stateless)\n",
        "        # We use the previous simple logic for local to avoid errors\n",
        "        if callable(chain) and not hasattr(chain, \"invoke\"):\n",
        "            return chain(question)\n",
        "        return chain.invoke(question)\n",
        "\n",
        "print(f\"üß† Chatbot with Memory is ready (Session ID: {session_id}).\")\n",
        "print(\"Try asking a question, then a follow-up like 'Telis l me more about that'.\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        q = input(\"\\nAsk a question (or press Enter to exit): \").strip()\n",
        "        if not q:\n",
        "            break\n",
        "        print(\"\\n--- Answer ---\")\n",
        "        print(ask_with_memory(q))\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nChat session ended.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20b49be3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4583ac10",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "45f63c70",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "67e8a725",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a0a1c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# ## 1) Install dependencies\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "# In a .py script, you would typically run these from your terminal first:\n",
        "# !pip -q install langchain langchain-community chromadb sentence-transformers\n",
        "# !pip -q install transformers accelerate\n",
        "# !pip -q install langchain-openai\n",
        "\n",
        "\n",
        "# ## 2) Load API Key\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# *** UPDATE THIS PATH to your .env file ***\n",
        "env_path = Path(\"/Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/ATT81022.env\")\n",
        "load_dotenv(dotenv_path=env_path)\n",
        "\n",
        "\n",
        "# ## 3) Set Constants & Check Key\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Path where Chroma (vector DB) will be persisted\n",
        "CHROMA_DIR = \"./chroma\"\n",
        "COLLECTION = \"uploaded_text\"\n",
        "\n",
        "# --- Optional: OpenAI ---\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n",
        "USE_OPENAI = bool(OPENAI_API_KEY)\n",
        "\n",
        "if USE_OPENAI:\n",
        "    print(\"‚úÖ Using OpenAI for generation.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è OPENAI_API_KEY not set ‚Äî will use local Transformers fallback.\")\n",
        "\n",
        "Path(CHROMA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "print(\"CHROMA_DIR =\", Path(CHROMA_DIR).resolve())\n",
        "print(\"COLLECTION  =\", COLLECTION)\n",
        "\n",
        "\n",
        "# ## 4) Load Text Document\n",
        "\n",
        "# *** UPDATE THIS PATH to your .txt file ***\n",
        "uploaded_path = \"/Volumes/Untitled/Youtube_QA_Rag_System/Working_Pipelines/text/RAG_TEXT.txt\"\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(uploaded_path).expanduser()\n",
        "assert p.exists(), f\"File not found: {p}\"\n",
        "\n",
        "text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "print(f\"Loaded {len(text):,} characters from:\", p.resolve())\n",
        "\n",
        "\n",
        "# ## 5) Define LLM (Generator)\n",
        "\n",
        "generator = None\n",
        "\n",
        "if USE_OPENAI:\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    generator = llm\n",
        "    print(\"Using ChatOpenAI: gpt-4o-mini\")\n",
        "else:\n",
        "    # Local Transformers text2text generation via HF pipeline\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "    print(\"Loading local model: google/flan-t5-base...\")\n",
        "    model_id = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "    hf_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    class HFText2TextLLM:\n",
        "        def __call__(self, prompt_text: str) -> str:\n",
        "            out = hf_pipe(prompt_text, max_new_tokens=256, truncation=True)\n",
        "            return out[0][\"generated_text\"]\n",
        "    \n",
        "    generator = HFText2TextLLM()\n",
        "    print(\"Using local Transformers: flan-t5-base\")\n",
        "\n",
        "\n",
        "# ## 6) Chunk, Embed, and Store in Vector DB\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# 1) Chunk the text\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        ")\n",
        "docs = [Document(page_content=c, metadata={\"source\": str(p.name)}) \n",
        "        for c in splitter.split_text(text)]\n",
        "print(f\"Chunks created: {len(docs)}\")\n",
        "\n",
        "# 2) Embedding function\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# 3) Create (or re-open) the Chroma collection\n",
        "vs = Chroma(\n",
        "    collection_name=COLLECTION,\n",
        "    persist_directory=CHROMA_DIR,\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "# 4) Add docs\n",
        "vs.add_documents(docs)\n",
        "print(\"‚úÖ Stored in Chroma at:\", Path(CHROMA_DIR).resolve())\n",
        "\n",
        "# 5) Create the retriever\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"\\n‚úÖ Created 'retriever' variable.\")\n",
        "\n",
        "\n",
        "# ## 7) Build RAG Chain\n",
        "\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_docs(docs):\n",
        "    out = []\n",
        "    for i, d in enumerate(docs):\n",
        "        src = d.metadata.get(\"source\", \"\")\n",
        "        out.append(f\"[{i}] {d.page_content}\\n(source: {src})\")\n",
        "    return \"\\n\\n\".join(out)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful assistant. Answer the question **only** from the provided context.however you can give general friend conversations \"\n",
        "    \"If the answer isn't present, say: 'I don't see that in the file.'\"\n",
        "      \"store the last 2 questions for retreval '\"\n",
        "     \"give full context to make the user understand '\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"{system}\"),\n",
        "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer succinctly:\"),\n",
        "])\n",
        "\n",
        "# Build chain\n",
        "if USE_OPENAI:\n",
        "    chain = (\n",
        "        RunnableParallel({\n",
        "            \"context\": (retriever | format_docs),\n",
        "            \"question\": RunnablePassthrough(),\n",
        "            \"system\": (lambda _: SYSTEM_PROMPT),\n",
        "        })\n",
        "        | prompt\n",
        "        | generator\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    print(\"‚úÖ RAG chain (OpenAI) is ready.\")\n",
        "else:\n",
        "    # Emulate the same behavior for the local model in a function\n",
        "    def answer_local(question: str) -> str:\n",
        "        ctx = format_docs(retriever.get_relevant_documents(question))\n",
        "        full_prompt = (\n",
        "            f\"{SYSTEM_PROMPT}\\n\\n\"\n",
        "            f\"Question: {question}\\n\\n\"\n",
        "            f\"Context:\\n{ctx}\\n\\n\"\n",
        "            f\"Answer succinctly:\"\n",
        "        )\n",
        "        return generator(full_prompt)\n",
        "\n",
        "    chain = answer_local\n",
        "    print(\"‚úÖ RAG function (Local Transformers) is ready.\")\n",
        "\n",
        "\n",
        "# ## 8) Ask Questions (Interactive)\n",
        "\n",
        "# Define the 'ask' function\n",
        "def ask(question: str):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a non-empty question.\"\n",
        "    if callable(chain) and not hasattr(chain, \"invoke\"):\n",
        "        # Local HF function path\n",
        "        return chain(question)\n",
        "    # OpenAI path via LangChain\n",
        "    return chain.invoke(question)\n",
        "\n",
        "\n",
        "# ---\n",
        "# ## 9) Evaluation Module\n",
        "# ---\n",
        "\n",
        "# 1. Define your Test Set\n",
        "# *** UPDATE THIS TEST SET with questions and answers relevant to your document ***\n",
        "\n",
        "evaluation_test_set = [\n",
        "    {\n",
        "        \"question\": \"What is the file about?\",\n",
        "        \"ground_truth_answer\": \"The file is a comprehensive guide for travelers exploring Europe, focusing on its diverse cultural and natural landscapes, history, art, and the blend of tradition and modernity.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the Netherlands known for?\",\n",
        "        \"ground_truth_answer\": \"The Netherlands is known for its canals, tulip fields, and cycling culture. Key attractions include Amsterdam‚Äôs Rijksmuseum and Anne Frank House.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Where is the Netherlands located?\",\n",
        "        \"ground_truth_answer\": \"The Netherlands is located in Western Europe.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the capital of Spain?\",\n",
        "        \"ground_truth_answer\": \"The document mentions Spain's cultural heritage, architecture, and historical sites, but it does not explicitly state the capital city.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(evaluation_test_set)} evaluation questions.\")\n",
        "\n",
        "\n",
        "# 2. Define the Evaluation Function (LLM-as-Judge)\n",
        "\n",
        "EVAL_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert evaluator for a Question-Answering system. \n",
        "Your goal is to assess whether the 'Generated Answer' correctly and faithfully answers the 'User Question' based *only* on the 'Ground Truth Answer'.\n",
        "\n",
        "RULES:\n",
        "- If the 'Generated Answer' is consistent with, and supported by, the 'Ground Truth Answer', respond with **CORRECT**.\n",
        "- If the 'Generated Answer' contradicts, fabricates information, or misses the main point of the 'Ground Truth Answer', respond with **INCORRECT**.\n",
        "- If the 'Generated Answer' is something like 'I don't see that in the file' and the 'Ground Truth Answer' also indicates the information is missing, this is **CORRECT**.\n",
        "\n",
        "--- EXAMPLES ---\n",
        "User Question: What is the capital of France?\n",
        "Ground Truth Answer: Paris is the capital of France.\n",
        "Generated Answer: The capital of France is Paris.\n",
        "Assessment: CORRECT\n",
        "\n",
        "User Question: What is the capital of France?\n",
        "Ground Truth Answer: Paris is the capital of France.\n",
        "Generated Answer: I don't see that in the file.\n",
        "Assessment: INCORRECT\n",
        "\n",
        "User Question: What is the capital of Mars?\n",
        "Ground Truth Answer: The document does not mention the capital of Mars.\n",
        "Generated Answer: I don't see that in the file.\n",
        "Assessment: CORRECT\n",
        "--- END EXAMPLES ---\n",
        "\n",
        "Provide only the final assessment ('CORRECT' or 'INCORRECT').\n",
        "\n",
        "--- TASK ---\n",
        "User Question: {question}\n",
        "Ground Truth Answer: {ground_truth}\n",
        "Generated Answer: {generated_answer}\n",
        "\n",
        "Assessment:\"\"\"\n",
        "\n",
        "eval_prompt = ChatPromptTemplate.from_template(EVAL_PROMPT_TEMPLATE)\n",
        "\n",
        "# Note: We re-use the 'generator' (LLM) from Cell 5 as our judge\n",
        "if USE_OPENAI:\n",
        "    evaluation_chain = (\n",
        "        eval_prompt\n",
        "        | generator \n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    print(\"‚úÖ LLM-as-Judge (OpenAI) is ready.\")\n",
        "else:\n",
        "    # Local models require the full prompt to be built manually\n",
        "    def eval_local(inputs: dict) -> str:\n",
        "        full_prompt = eval_prompt.format(**inputs)\n",
        "        return generator(full_prompt)\n",
        "    \n",
        "    evaluation_chain = eval_local\n",
        "    print(\"‚úÖ LLM-as-Judge (Local Transformers) is ready.\")\n",
        "\n",
        "\n",
        "def evaluate_pipeline():\n",
        "    print(\"Running evaluation...\")\n",
        "    print(\"=\"*30)\n",
        "    \n",
        "    correct"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Rag_Env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
