{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed026c28",
      "metadata": {},
      "source": [
        "# RAG Pipeline for Q&A over a Text File\n",
        "\n",
        "This notebook implements a clean Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "1.  **Install** required libraries.\n",
        "2.  **Load** an `OPENAI_API_KEY` (if available).\n",
        "3.  **Load** a source `.txt` file.\n",
        "4.  **Chunk, Embed, & Store** the text in a Chroma vector database.\n",
        "5.  **Build** a LangChain RAG chain to answer questions.\n",
        "6.  **Run** an interactive chat loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bf1e322f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.10.19 (main, Oct 21 2025, 16:37:10) [Clang 20.1.8 ]\n"
          ]
        }
      ],
      "source": [
        "## 1) Install dependencies\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "# Core libs\n",
        "!pip -q install langchain langchain-community chromadb sentence-transformers\n",
        "\n",
        "# For optional local LLM fallback\n",
        "!pip -q install transformers accelerate\n",
        "\n",
        "# For OpenAI\n",
        "!pip -q install langchain-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "29c4f426",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## 2) Load API Key\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# *** UPDATE THIS PATH to your .env file ***\n",
        "env_path = Path(\"/Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/ATT81022.env\")\n",
        "load_dotenv(dotenv_path=env_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "027d7163",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Using OpenAI for generation.\n",
            "CHROMA_DIR = /Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/chroma\n",
            "COLLECTION  = uploaded_text\n"
          ]
        }
      ],
      "source": [
        "## 3) Set Constants & Check Key\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Path where Chroma (vector DB) will be persisted\n",
        "CHROMA_DIR = \"./chroma\"\n",
        "COLLECTION = \"uploaded_text\"\n",
        "\n",
        "# --- Optional: OpenAI ---\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n",
        "USE_OPENAI = bool(OPENAI_API_KEY)\n",
        "\n",
        "if USE_OPENAI:\n",
        "    print(\"‚úÖ Using OpenAI for generation.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è OPENAI_API_KEY not set ‚Äî will use local Transformers fallback.\")\n",
        "\n",
        "Path(CHROMA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "print(\"CHROMA_DIR =\", Path(CHROMA_DIR).resolve())\n",
        "print(\"COLLECTION  =\", COLLECTION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f46b1af8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10,135 characters from: /Volumes/Untitled/Youtube_QA_Rag_System/Working_Pipelines/text/RAG_TEXT.txt\n"
          ]
        }
      ],
      "source": [
        "## 4) Load Text Document\n",
        "\n",
        "# *** UPDATE THIS PATH to your .txt file ***\n",
        "uploaded_path = \"/Volumes/Untitled/Youtube_QA_Rag_System/Working_Pipelines/text/RAG_TEXT.txt\"\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(uploaded_path).expanduser()\n",
        "assert p.exists(), f\"File not found: {p}\"\n",
        "\n",
        "text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "print(f\"Loaded {len(text):,} characters from:\", p.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "27a7dcdf",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using ChatOpenAI: gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "## 5) Define LLM (Generator)\n",
        "\n",
        "generator = None\n",
        "\n",
        "if USE_OPENAI:\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    generator = llm\n",
        "    print(\"Using ChatOpenAI: gpt-4o-mini\")\n",
        "else:\n",
        "    # Local Transformers text2text generation via HF pipeline\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "    print(\"Loading local model: google/flan-t5-base...\")\n",
        "    model_id = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "    hf_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    class HFText2TextLLM:\n",
        "        def __call__(self, prompt_text: str) -> str:\n",
        "            out = hf_pipe(prompt_text, max_new_tokens=256, truncation=True)\n",
        "            return out[0][\"generated_text\"]\n",
        "    \n",
        "    generator = HFText2TextLLM()\n",
        "    print(\"Using local Transformers: flan-t5-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5820b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "COLLECTION = \"uploaded_text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "182a66aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks created: 55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/59/_rmmbn7d6kx3y2yg0wqg8ghc0000gn/T/ipykernel_69548/2229221155.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n",
            "/var/folders/59/_rmmbn7d6kx3y2yg0wqg8ghc0000gn/T/ipykernel_69548/2229221155.py:25: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vs = Chroma(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Stored in Chroma at: /Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/chroma\n",
            "\n",
            "‚úÖ Created 'retriever' variable.\n"
          ]
        }
      ],
      "source": [
        "## 6) Chunk, Embed, and Store in Vector DB\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# 1) Chunk the text\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        ")\n",
        "docs = [Document(page_content=c, metadata={\"source\": str(p.name)}) \n",
        "        for c in splitter.split_text(text)]\n",
        "print(f\"Chunks created: {len(docs)}\")\n",
        "\n",
        "# 2) Embedding function\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# 3) Create (or re-open) the Chroma collection\n",
        "vs = Chroma(\n",
        "    collection_name=COLLECTION,\n",
        "    persist_directory=CHROMA_DIR,\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "# 4) Add docs\n",
        "vs.add_documents(docs)\n",
        "print(\"‚úÖ Stored in Chroma at:\", Path(CHROMA_DIR).resolve())\n",
        "\n",
        "# 5) Create the retriever\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"\\n‚úÖ Created 'retriever' variable.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "45af9258",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Multi-Query Retriever (OpenAI) is active.\n",
            "   (The system will now generate variations of your question for better search results.)\n"
          ]
        }
      ],
      "source": [
        "## 6.5) Upgrade to Multi-Query Retriever\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "import logging\n",
        "\n",
        "# Optional: Turn on logging so you can see the different questions the AI generates\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "if USE_OPENAI:\n",
        "    # This uses the LLM (gpt-4o-mini) to generate variations of the question\n",
        "    # and retrieve documents for all variations.\n",
        "    retriever = MultiQueryRetriever.from_llm(\n",
        "        retriever=vs.as_retriever(search_kwargs={\"k\": 5}),\n",
        "        llm=llm\n",
        "    )\n",
        "    print(\"‚úÖ Multi-Query Retriever (OpenAI) is active.\")\n",
        "    print(\"   (The system will now generate variations of your question for better search results.)\")\n",
        "\n",
        "else:\n",
        "    # Fallback: Multi-query requires a strong instruction-following LLM.\n",
        "    # Smaller local models (like flan-t5) often fail the strict formatting requirements.\n",
        "    retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "    print(\"‚ÑπÔ∏è Using standard retriever (Local Model).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "af27e405",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Conversational RAG (OpenAI + Memory) is ready.\n"
          ]
        }
      ],
      "source": [
        "## 7) Build Conversational RAG Chain (With Memory)\n",
        "\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# --- 1. Contextualize Question ---\n",
        "# This prompt helps the LLM understand follow-up questions (e.g., \"What about him?\")\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", contextualize_q_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# --- 2. Answer Question ---\n",
        "# This is the prompt that actually answers the user\n",
        "qa_system_prompt = (\n",
        "    \"You are a helpful assistant. Answer the question only from the provided context. \"\n",
        "    \"If the answer isn't present, say: 'I don't see that in the file nut try not to make it too obvious that you are ai you can play with differenct option ' \"\n",
        "    \"You may engage in friendly conversation, but never fabricate facts outside the context. \"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", qa_system_prompt),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# --- 3. Build the Chain ---\n",
        "if USE_OPENAI:\n",
        "    # Create a retriever that can handle history\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm, retriever, contextualize_q_prompt\n",
        "    )\n",
        "    \n",
        "    # Create the document combining chain\n",
        "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "    \n",
        "    # Combine them\n",
        "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "    # --- 4. Memory Management ---\n",
        "    store = {}\n",
        "\n",
        "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "        if session_id not in store:\n",
        "            store[session_id] = ChatMessageHistory()\n",
        "        return store[session_id]\n",
        "\n",
        "    conversational_chain = RunnableWithMessageHistory(\n",
        "        rag_chain,\n",
        "        get_session_history,\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "        output_messages_key=\"answer\",\n",
        "    )\n",
        "    print(\"‚úÖ Conversational RAG (OpenAI + Memory) is ready.\")\n",
        "\n",
        "else:\n",
        "    # Local Fallback (Simplified memory for local testing)\n",
        "    # Note: Local models often struggle with the complex history-rewriting step\n",
        "    print(\"‚ÑπÔ∏è Memory is disabled for local fallback to ensure stability.\")\n",
        "    conversational_chain = None \n",
        "    # We will handle the fallback logic in the ask function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b23737",
      "metadata": {},
      "source": [
        "## 8) Ask Questions\n",
        "\n",
        "Run the cells below to interact with your RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "46401dbb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† Chatbot with Memory is ready (Session ID: user_session_1).\n",
            "Try asking a question, then a follow-up like 'Tell me more about that'.\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['Is the Netherlands a good place to live or visit?  ', 'What are the pros and cons of living in the Netherlands?  ', 'How does the quality of life in the Netherlands compare to other countries?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't see that in the file, but it does mention that the Netherlands delights with its canals, tulip fields, and cycling culture, which suggests it has many appealing features. Would you like to know more about a specific aspect of the Netherlands?\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the meaning or significance of the name Kayode?  ', 'Can you explain the cultural or historical background of the name Kayode?  ', 'What are the origins and interpretations associated with the name Kayode?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi Kayode! It's nice to meet you. How can I assist you today?\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Nj·∫π o le ·π£e alaye ni ede Yoruba?', '2. ·π¢e o le k·ªç ni Yoruba?', '3. Nj·∫π o le ba mi s·ªçr·ªç ni Yoruba?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't see that in the file, but I can help with other questions or topics if you'd like!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the geographical position of the Netherlands?  ', 'Can you tell me the location of the Netherlands on a map?  ', 'In which part of Europe can I find the Netherlands?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't see that in the file, but I can tell you that NL typically refers to the Netherlands. If you have more questions about it, feel free to ask!\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What was the question you asked before this one?  ', 'Can you remind me of the question you posed earlier?  ', 'What did you inquire about in your last message?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your previous question was about whether the Netherlands is good.\n",
            "\n",
            "--- Answer ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: [\"Can you recall previous conversations or interactions we've had?  \", 'Are you capable of retaining information from our past discussions?  ', 'Do you have the functionality to remember our earlier exchanges?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't have memory in the way humans do, but I can keep track of our conversation while we're chatting. Once the conversation ends, I won't remember anything from it. How can I assist you further?\n"
          ]
        }
      ],
      "source": [
        "## 8 & 9) Chat with Memory\n",
        "\n",
        "# Create a random session ID for this run\n",
        "session_id = \"user_session_1\"\n",
        "\n",
        "def ask_with_memory(question: str):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a non-empty question.\"\n",
        "    \n",
        "    if USE_OPENAI:\n",
        "        # Invoke with session_id so it remembers previous turns\n",
        "        response = conversational_chain.invoke(\n",
        "            {\"input\": question},\n",
        "            config={\"configurable\": {\"session_id\": session_id}}\n",
        "        )\n",
        "        return response[\"answer\"]\n",
        "    else:\n",
        "        # Fallback for local model (Stateless)\n",
        "        # We use the previous simple logic for local to avoid errors\n",
        "        if callable(chain) and not hasattr(chain, \"invoke\"):\n",
        "            return chain(question)\n",
        "        return chain.invoke(question)\n",
        "\n",
        "print(f\"üß† Chatbot with Memory is ready (Session ID: {session_id}).\")\n",
        "print(\"Try asking a question, then a follow-up like 'Tell me more about that'.\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        q = input(\"\\nAsk a question (or press Enter to exit): \").strip()\n",
        "        if not q:\n",
        "            break\n",
        "        print(\"\\n--- Answer ---\")\n",
        "        print(ask_with_memory(q))\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nChat session ended.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Rag_Env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
