{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe6f019",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ“˜ RAG over a Manually Uploaded Text File (Chroma in Working Directory)\n",
    "\n",
    "This notebook lets you:\n",
    "1. **Upload a `.txt` file manually** (Colab or local Jupyter).\n",
    "2. **Chunk & embed** the text with `sentence-transformers`.\n",
    "3. **Store** vectors in a **Chroma** DB saved to `./chroma` (working directory).\n",
    "4. **Ask questions**: a simple **RAG** flow retrieves top snippets and answers via:\n",
    "   - **OpenAI** (if `OPENAI_API_KEY` is set), or\n",
    "   - **Local Transformers** fallback (`google/flan-t5-base`) if no OpenAI key.\n",
    "   \n",
    "> Works on **Colab** and **Windows/macOS** Jupyter. No `.env`/dotenv needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04344fd7",
   "metadata": {},
   "source": [
    "## 0) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbd6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "# Core libs\n",
    "!pip -q install langchain langchain-community chromadb sentence-transformers\n",
    "\n",
    "# For optional local LLM fallback\n",
    "!pip -q install transformers accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c82e7",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb6a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using OpenAI for generation.\n",
      "CHROMA_DIR = /Volumes/Untitled/Lessons_By_Week/Youtube_Project/Working_Pipelines/chroma\n",
      "COLLECTION  = uploaded_text\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Path where Chroma (vector DB) will be persisted ---\n",
    "CHROMA_DIR = \"./chroma\"   # working directory\n",
    "COLLECTION = \"uploaded_text\"\n",
    "\n",
    "# --- Optional: OpenAI ---\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n",
    "USE_OPENAI = bool(OPENAI_API_KEY)\n",
    "if USE_OPENAI:\n",
    "    print(\"âœ… Using OpenAI for generation.\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ OPENAI_API_KEY not set â€” will use local Transformers fallback.\")\n",
    "\n",
    "Path(CHROMA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"CHROMA_DIR =\", Path(CHROMA_DIR).resolve())\n",
    "print(\"COLLECTION  =\", COLLECTION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf99a56",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Upload your `.txt` file\n",
    "\n",
    "Choose **one** method below:\n",
    "\n",
    "- **A. Google Colab uploader** â€“ pick a file from your computer.\n",
    "- **B. Local path** â€“ type/paste an existing path on your machine.\n",
    "- **C. Jupyter widget** â€“ drag & drop in classic Jupyter (optional).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dfb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A) Google Colab uploader\n",
    "# If you're in Colab, run this cell and select a .txt file.\n",
    "# Otherwise, skip to method B or C.\n",
    "\n",
    "uploaded_path = None\n",
    "try:\n",
    "    from google.colab import files  # type: ignore\n",
    "    up = files.upload()  # opens file chooser\n",
    "    if up:\n",
    "        fname = list(up.keys())[0]\n",
    "        uploaded_path = fname\n",
    "        print(\"Uploaded:\", uploaded_path)\n",
    "except Exception as e:\n",
    "    # Not in Colab, ignore\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccc9e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected path: /Volumes/Untitled/Lessons_By_Week/Youtube_Project/QA_YOUTUBE_NOTEBOOK/Building_Codes/RAG_TEXT.txt\n"
     ]
    }
   ],
   "source": [
    "# B) Local path (works anywhere)\n",
    "manual_path = \"/Volumes/Untitled/Lessons_By_Week/Youtube_Project/QA_YOUTUBE_NOTEBOOK/Building_Codes/RAG_TEXT.txt\"\n",
    "\n",
    "if manual_path:\n",
    "    uploaded_path = manual_path\n",
    "\n",
    "print(\"Selected path:\", uploaded_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c77fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# C) Jupyter file upload widget (optional; classic Jupyter/Lab)\n",
    "uploaded_via_widget = True\n",
    "try:\n",
    "    import io, ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    fu = widgets.FileUpload(accept='.txt', multiple=False)\n",
    "    display(fu)\n",
    "\n",
    "    def _capture(change):\n",
    "        global uploaded_via_widget, uploaded_path\n",
    "        if len(fu.value):\n",
    "            meta = list(fu.value.values())[0]\n",
    "            content = meta['content']\n",
    "            name = meta['metadata']['name']\n",
    "            with open(name, 'wb') as f:\n",
    "                f.write(content)\n",
    "            uploaded_via_widget = name\n",
    "            uploaded_path = name\n",
    "            print(\"Widget uploaded:\", uploaded_path)\n",
    "\n",
    "    fu.observe(_capture, 'value')\n",
    "except Exception as e:\n",
    "    # ipywidgets not installed or not supported; ignore\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ec9d3",
   "metadata": {},
   "source": [
    "## 3) Read & clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca05007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 110,700 characters from: /Volumes/Untitled/Lessons_By_Week/Youtube_Project/QA_YOUTUBE_NOTEBOOK/Building_Codes/RAG_TEXT.txt\n",
      "First 500 chars:\n",
      " ===== [1] Delivery â€“ Netflix | Partner Help Center :: https://partnerhelp.netflixstudios.com/hc/en-us/categories/1500000000761-Delivery =====\n",
      "html\n",
      "Delivery â€“ Netflix | Partner Help Center\n",
      "Return to Help Center\n",
      "Partner Help Center\n",
      "Sign in\n",
      "Menu\n",
      "Delivery\n",
      "Netflix | Partner Help Center\n",
      "Delivery\n",
      "Delivery Specifications\n",
      "+ Branded Delivery Specifications\n",
      "+ Non-Branded Delivery Specifications\n",
      "Near Field 2.0 Stereo Delivery Specifications and Guidelines for Titles Mixed Natively in 2.0\n",
      "Non Graded Archival\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "assert uploaded_path, \"No file selected. Use one of the upload methods above.\"\n",
    "\n",
    "p = Path(uploaded_path)\n",
    "assert p.exists(), f\"File not found: {p}\"\n",
    "\n",
    "text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "print(f\"Loaded {len(text):,} characters from:\", p.resolve())\n",
    "\n",
    "# Light cleanup\n",
    "text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "text = \"\\n\".join([line.strip() for line in text.split(\"\\n\")])\n",
    "print(\"First 500 chars:\\n\", text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ddaab",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Chatbot (Python 3.11.14)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n Chatbot ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Pre-chunking similarity analysis ---\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_document_similarity(text, sample_size=10, window_size=3):\n",
    "    \"\"\"\n",
    "    Analyze similarity between different parts of the document before chunking\n",
    "    \"\"\"\n",
    "    # Split into paragraphs/sections first\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    paragraphs = paragraphs[:sample_size]  # Analyze first N paragraphs\n",
    "    \n",
    "    print(f\"Analyzing similarity between first {len(paragraphs)} paragraphs...\")\n",
    "    \n",
    "    # Embed each paragraph\n",
    "    paragraph_embeddings = []\n",
    "    valid_paragraphs = []\n",
    "    \n",
    "    for i, para in enumerate(paragraphs):\n",
    "        if len(para) > 50:  # Only embed meaningful paragraphs\n",
    "            embedding = embeddings.embed_query(para)\n",
    "            paragraph_embeddings.append(embedding)\n",
    "            valid_paragraphs.append((i, para[:100] + \"...\"))  # Store preview\n",
    "    \n",
    "    if len(paragraph_embeddings) < 2:\n",
    "        print(\"Not enough meaningful paragraphs for similarity analysis\")\n",
    "        return\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = cosine_similarity(paragraph_embeddings)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Similarity Matrix (first 5x5):\")\n",
    "    print(\"Rows/Columns represent paragraph indices\")\n",
    "    print(np.round(similarity_matrix[:5, :5], 3))\n",
    "    \n",
    "    # Find most similar and least similar pairs\n",
    "    n = len(similarity_matrix)\n",
    "    most_similar = (0, 1, similarity_matrix[0, 1])\n",
    "    least_similar = (0, 1, similarity_matrix[0, 1])\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            sim = similarity_matrix[i, j]\n",
    "            if sim > most_similar[2]:\n",
    "                most_similar = (i, j, sim)\n",
    "            if sim < least_similar[2]:\n",
    "                least_similar = (i, j, sim)\n",
    "    \n",
    "    print(f\"\\nðŸ”— Most similar paragraphs: {most_similar[0]} & {most_similar[1]}\")\n",
    "    print(f\"   Similarity score: {most_similar[2]:.3f}\")\n",
    "    print(f\"   Para {most_similar[0]}: {valid_paragraphs[most_similar[0]][1]}\")\n",
    "    print(f\"   Para {most_similar[1]}: {valid_paragraphs[most_similar[1]][1]}\")\n",
    "    \n",
    "    print(f\"\\nðŸš« Least similar paragraphs: {least_similar[0]} & {least_similar[1]}\")\n",
    "    print(f\"   Similarity score: {least_similar[2]:.3f}\")\n",
    "    print(f\"   Para {least_similar[0]}: {valid_paragraphs[least_similar[0]][1]}\")\n",
    "    print(f\"   Para {least_similar[1]}: {valid_paragraphs[least_similar[1]][1]}\")\n",
    "    \n",
    "    # Visualize similarity matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    plt.title('Document Paragraph Similarity Matrix')\n",
    "    plt.xlabel('Paragraph Index')\n",
    "    plt.ylabel('Paragraph Index')\n",
    "    plt.show()\n",
    "    \n",
    "    return similarity_matrix, valid_paragraphs\n",
    "\n",
    "# Run the analysis\n",
    "similarity_matrix, paragraphs_info = analyze_document_similarity(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db252d",
   "metadata": {},
   "source": [
    "## 4) Chunk â†’ Embed â†’ Persist to Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "821696d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created: 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/_rmmbn7d6kx3y2yg0wqg8ghc0000gn/T/ipykernel_7246/3970925276.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/var/folders/59/_rmmbn7d6kx3y2yg0wqg8ghc0000gn/T/ipykernel_7246/3970925276.py:27: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vs = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Stored in Chroma at: /Volumes/Untitled/Lessons_By_Week/Youtube_Project/Working_Pipelines/chroma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/_rmmbn7d6kx3y2yg0wqg8ghc0000gn/T/ipykernel_7246/3970925276.py:57: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vs.persist()  # no-op on some versions; safe to call if present\n"
     ]
    }
   ],
   "source": [
    "# --- Chunk â†’ Embed â†’ Persist to Chroma (version-safe) ---\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 1) Chunk the text\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=160,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")\n",
    "docs = [Document(page_content=c, metadata={\"source\": str(p.name)}) \n",
    "        for c in splitter.split_text(text)]\n",
    "print(f\"Chunks created: {len(docs)}\")\n",
    "\n",
    "# 2) Embedding function (no manual embedding arrays)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "# 3) Create (or re-open) the Chroma collection in the working dir\n",
    "#    NOTE: We pass `embedding_function=embeddings` so vectorstore handles embeddings internally.\n",
    "vs = Chroma(\n",
    "    collection_name=COLLECTION,\n",
    "    persist_directory=CHROMA_DIR,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "# 4) (Optional) Clear previous contents of this collection\n",
    "#    Easiest cross-version reset: use a fresh collection name, or delete the folder manually.\n",
    "#    If you really want programmatic clearing, uncomment:\n",
    "# try:\n",
    "#     # Recreate an empty collection (requires chromadb>=0.4 with delete support)\n",
    "#     from chromadb import PersistentClient\n",
    "#     client = PersistentClient(path=CHROMA_DIR)\n",
    "#     try:\n",
    "#         client.delete_collection(COLLECTION)\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     vs = Chroma(\n",
    "#         collection_name=COLLECTION,\n",
    "#         persist_directory=CHROMA_DIR,\n",
    "#         embedding_function=embeddings,\n",
    "#     )\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# 5) Add docs (embeddings computed under the hood)\n",
    "vs.add_documents(docs)\n",
    "\n",
    "# 6) Persist if available (older langchain-Chroma exposes .persist(); newer chromadb persists automatically)\n",
    "try:\n",
    "    vs.persist()  # no-op on some versions; safe to call if present\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"âœ… Stored in Chroma at:\", Path(CHROMA_DIR).resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb867c",
   "metadata": {},
   "source": [
    "## 5) Build retriever & choose generator (OpenAI or local Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec39a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG chain (OpenAI) is ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Reopen collection (simulates separate session)\n",
    "vs = Chroma(collection_name=COLLECTION, persist_directory=CHROMA_DIR)\n",
    "\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "def format_docs(docs):\n",
    "    out = []\n",
    "    for i, d in enumerate(docs):\n",
    "        src = d.metadata.get(\"source\", \"\")\n",
    "        out.append(f\"[{i}] {d.page_content}\\n(source: {src})\")\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant. Answer the question **only** from the provided context. \"\n",
    "    \"If the answer isn't present, say: 'KAY SAYS I don't see that in the file.'\"\n",
    "     \"give full context to make the user understand '\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"{system}\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer succinctly:\"),\n",
    "])\n",
    "\n",
    "# --- Choose generator ---\n",
    "generator = None\n",
    "if USE_OPENAI:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    generator = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "else:\n",
    "    # Local Transformers text2text generation via HF pipeline\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "    model_id = \"google/flan-t5-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "    hf_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    class HFText2TextLLM:\n",
    "        def __call__(self, prompt_text: str) -> str:\n",
    "            out = hf_pipe(prompt_text, max_new_tokens=256, truncation=True)\n",
    "            return out[0][\"generated_text\"]\n",
    "\n",
    "    # Wrap prompt execution manually when using local model\n",
    "    generator = HFText2TextLLM()\n",
    "\n",
    "# Build chain (LangChain-style if OpenAI; manual if local HF)\n",
    "if USE_OPENAI:\n",
    "    chain = (\n",
    "        RunnableParallel({\n",
    "            \"context\": (retriever | format_docs),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"system\": (lambda _: SYSTEM_PROMPT),\n",
    "        })\n",
    "        | prompt\n",
    "        | generator\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    print(\"âœ… RAG chain (OpenAI) is ready.\")\n",
    "else:\n",
    "    # We'll emulate the same behavior for the local model in a function\n",
    "    def answer_local(question: str) -> str:\n",
    "        ctx = format_docs(retriever.get_relevant_documents(question))\n",
    "        full_prompt = (\n",
    "            f\"{SYSTEM_PROMPT}\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Context:\\n{ctx}\\n\\n\"\n",
    "            f\"Answer succinctly:\"\n",
    "        )\n",
    "        return generator(full_prompt)\n",
    "\n",
    "    chain = answer_local\n",
    "    print(\"âœ… RAG function (Local Transformers) is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced with Simple Memory ---\n",
    "\n",
    "class SimpleMemory:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def add(self, question, answer):\n",
    "        self.conversation_history.append(f\"User: {question}\")\n",
    "        self.conversation_history.append(f\"Assistant: {answer}\")\n",
    "        # Keep last 6 exchanges\n",
    "        if len(self.conversation_history) > 12:\n",
    "            self.conversation_history = self.conversation_history[-12:]\n",
    "    \n",
    "    def get_context(self):\n",
    "        if not self.conversation_history:\n",
    "            return \"\"\n",
    "        return \"Conversation history:\\n\" + \"\\n\".join(self.conversation_history[-6:]) + \"\\n\\n\"\n",
    "\n",
    "# Initialize memory\n",
    "memory = SimpleMemory()\n",
    "\n",
    "# Replace your existing ask function with this enhanced version:\n",
    "def ask(question: str):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a non-empty question.\"\n",
    "    \n",
    "    # Get memory context\n",
    "    memory_context = memory.get_context()\n",
    "    \n",
    "    if callable(chain) and not hasattr(chain, \"invoke\"):\n",
    "        # Local HF function path with memory\n",
    "        ctx = format_docs(retriever.get_relevant_documents(question))\n",
    "        enhanced_system_prompt = f\"{SYSTEM_PROMPT}\\n\\n{memory_context}\"\n",
    "        \n",
    "        full_prompt = (\n",
    "            f\"{enhanced_system_prompt}\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Context:\\n{ctx}\\n\\n\"\n",
    "            f\"Answer succinctly:\"\n",
    "        )\n",
    "        answer = generator(full_prompt)\n",
    "    else:\n",
    "        # OpenAI path with memory\n",
    "        enhanced_system_prompt = f\"{SYSTEM_PROMPT}\\n\\n{memory_context}\"\n",
    "        answer = chain.invoke({\n",
    "            \"system\": enhanced_system_prompt,\n",
    "            \"question\": question\n",
    "        })\n",
    "    \n",
    "    # Store in memory\n",
    "    memory.add(question, answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01326ebd",
   "metadata": {},
   "source": [
    "## 6) Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a non-empty question.\"\n",
    "    if callable(chain) and not hasattr(chain, \"invoke\"):\n",
    "        # Local HF function path\n",
    "        return chain(question)\n",
    "    # OpenAI path via LangChain\n",
    "    return chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8802cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file outlines guidelines for creating and delivering subtitles, forced narratives, and alternate language audio descriptions for content, ensuring that translations are accurate and adhere to specific formatting and technical requirements. It emphasizes the importance of maintaining the integrity of the original language while providing clear translations for non-native speakers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ask(question: str):\n",
    "    if not question.strip():\n",
    "        return \"Please enter a non-empty question.\"\n",
    "    if callable(chain) and not hasattr(chain, \"invoke\"):\n",
    "        # Local HF function path\n",
    "        return chain(question)\n",
    "    # OpenAI path via LangChain\n",
    "    return chain.invoke(question)\n",
    "\n",
    "# Try it:\n",
    "print(ask(\"Give me a 1â€“2 sentence summary of the file.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6b22c",
   "metadata": {},
   "source": [
    "## 7) (Optional) Interactive Q&A loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9f13706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Answer ---\n",
      " The text provides guidelines for creating and delivering textless versions of IMF (Interoperable Master Format) packages for Netflix. It outlines the process for handling forced narrative subtitles, which are necessary for conveying important dialogue or on-screen text without burned-in subtitles. The document emphasizes the need for a list of these forced narratives to be submitted using a specific template and details best practices for delivery.\n",
      "\n",
      "--- Answer ---\n",
      " The file contains guidelines and requirements for audio and video asset delivery, specifically focusing on Composition Playlist (CPL) XML files for different audio formats (Dolby Atmos, 5.1 surround, and 2.0 stereo), file naming conventions, and language codes for subtitles and audio descriptions. It outlines the necessary components of an IMF package, acceptable formats for alternate languages, and the importance of avoiding slurs in translations. Additionally, it includes a list of language codes for various languages supported by Netflix.\n",
      "\n",
      "--- Answer ---\n",
      " The components of the IMF package must include:\n",
      "\n",
      "1. Asset Map XML file.\n",
      "2. Packing List XML file.\n",
      "3. One or more Composition Playlist (CPL) XML files, each representing a single audio language, containing:\n",
      "   - For titles mixed in Dolby AtmosÂ®:\n",
      "     - 1 image virtual track.\n",
      "     - 1 IAB virtual track.\n",
      "     - 0 or 1 surround audio virtual track (6 channels).\n",
      "     - 0 or 1 stereo audio virtual track (2 channels).\n",
      "   - For titles mixed in 5.1 surround:\n",
      "     - 1 image virtual track.\n",
      "     - 1 surround audio virtual track (6 channels).\n",
      "     - 0 or 1 stereo audio virtual track (2 channels).\n",
      "   - For titles mixed in 2.0 stereo:\n",
      "     - 1 image virtual track.\n",
      "     - 1 stereo audio virtual track (2 channels).\n",
      "4. All MXF Track Files referenced from the ingested CPL that have not previously been delivered.\n",
      "5. Volume Index and Output Profile List XML files may be present but are not required.\n",
      "\n",
      "--- Answer ---\n",
      " KAY SAYS I don't see that in the file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run this cell to chat in the console.\n",
    "# Stop with Ctrl+C (or by interrupting the kernel).\n",
    "try:\n",
    "    while True:\n",
    "        q = input(\"\\nAsk a question (or press Enter to exit): \").strip()\n",
    "        if not q:\n",
    "            break\n",
    "        print(\"\\n--- Answer ---\\n\", ask(q))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0dfd5d",
   "metadata": {},
   "source": [
    "\n",
    "## Notes, Tips, and Tweaks\n",
    "\n",
    "- **Chunk sizes**: For dense documents, try `chunk_size=1000â€“1500`, `chunk_overlap=150â€“250`.\n",
    "- **Retriever depth**: Increase `k` to 8â€“10 for broader context.\n",
    "- **Chroma persistence**: Data is saved in `./chroma`. You can delete that folder to reset.\n",
    "- **Multiple files**: Re-run the *Chunk â†’ Embed â†’ Persist* cell with another file; use different `COLLECTION` names to keep them separate.\n",
    "- **Local model**: `google/flan-t5-base` is light but not perfect. For better local quality, swap it for a bigger model (e.g., `flan-t5-large`) if your machine/Colab GPU can handle it.\n",
    "- **OpenAI path**: If you set `OPENAI_API_KEY`, the notebook uses `gpt-4o-mini` for higher-quality answers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
