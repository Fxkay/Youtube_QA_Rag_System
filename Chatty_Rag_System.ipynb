{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed026c28",
      "metadata": {},
      "source": [
        "# RAG Pipeline for Q&A over a Text File\n",
        "\n",
        "This notebook implements a clean Retrieval-Augmented Generation (RAG) pipeline.\n",
        "\n",
        "1.  **Install** required libraries.\n",
        "2.  **Load** an `OPENAI_API_KEY` (if available).\n",
        "3.  **Load** a source `.txt` file.\n",
        "4.  **Chunk, Embed, & Store** the text in a Chroma vector database.\n",
        "5.  **Build** a LangChain RAG chain to answer questions.\n",
        "6.  **Run** an interactive chat loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bf1e322f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.10.19 (main, Oct 21 2025, 16:37:10) [Clang 20.1.8 ]\n"
          ]
        }
      ],
      "source": [
        "## 1) Install dependencies\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "# Core libs\n",
        "!pip -q install langchain langchain-community chromadb sentence-transformers\n",
        "\n",
        "# For optional local LLM fallback\n",
        "!pip -q install transformers accelerate\n",
        "\n",
        "# For OpenAI\n",
        "!pip -q install langchain-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "29c4f426",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## 2) Load API Key\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# *** UPDATE THIS PATH to your .env file ***\n",
        "env_path = Path(\"/Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/ATT81022.env\")\n",
        "load_dotenv(dotenv_path=env_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "027d7163",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Using OpenAI for generation.\n",
            "CHROMA_DIR = /Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/chroma\n",
            "COLLECTION  = uploaded_text\n"
          ]
        }
      ],
      "source": [
        "## 3) Set Constants & Check Key\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Path where Chroma (vector DB) will be persisted\n",
        "CHROMA_DIR = \"./chroma\"\n",
        "COLLECTION = \"uploaded_text\"\n",
        "\n",
        "# --- Optional: OpenAI ---\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\").strip()\n",
        "USE_OPENAI = bool(OPENAI_API_KEY)\n",
        "\n",
        "if USE_OPENAI:\n",
        "    print(\"✅ Using OpenAI for generation.\")\n",
        "else:\n",
        "    print(\"ℹ️ OPENAI_API_KEY not set — will use local Transformers fallback.\")\n",
        "\n",
        "Path(CHROMA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "print(\"CHROMA_DIR =\", Path(CHROMA_DIR).resolve())\n",
        "print(\"COLLECTION  =\", COLLECTION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f46b1af8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10,135 characters from: /Volumes/Untitled/Youtube_QA_Rag_System/Working_Pipelines/text/RAG_TEXT.txt\n"
          ]
        }
      ],
      "source": [
        "## 4) Load Text Document\n",
        "\n",
        "# *** UPDATE THIS PATH to your .txt file ***\n",
        "uploaded_path = \"/Volumes/Untitled/Youtube_QA_Rag_System/Working_Pipelines/text/RAG_TEXT.txt\"\n",
        "from pathlib import Path\n",
        "\n",
        "p = Path(uploaded_path).expanduser()\n",
        "assert p.exists(), f\"File not found: {p}\"\n",
        "\n",
        "text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "print(f\"Loaded {len(text):,} characters from:\", p.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "27a7dcdf",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/Rag_Env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using ChatOpenAI: gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "## 5) Define LLM (Generator)\n",
        "\n",
        "generator = None\n",
        "\n",
        "if USE_OPENAI:\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    generator = llm\n",
        "    print(\"Using ChatOpenAI: gpt-4o-mini\")\n",
        "else:\n",
        "    # Local Transformers text2text generation via HF pipeline\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "    print(\"Loading local model: google/flan-t5-base...\")\n",
        "    model_id = \"google/flan-t5-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "    hf_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    class HFText2TextLLM:\n",
        "        def __call__(self, prompt_text: str) -> str:\n",
        "            out = hf_pipe(prompt_text, max_new_tokens=256, truncation=True)\n",
        "            return out[0][\"generated_text\"]\n",
        "    \n",
        "    generator = HFText2TextLLM()\n",
        "    print(\"Using local Transformers: flan-t5-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f5820b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "COLLECTION = \"uploaded_text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "182a66aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks created: 55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/59/_rmmbn7d6kx3y2yg0wqg8ghc0000gn/T/ipykernel_69548/2229221155.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n",
            "/var/folders/59/_rmmbn7d6kx3y2yg0wqg8ghc0000gn/T/ipykernel_69548/2229221155.py:25: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vs = Chroma(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Stored in Chroma at: /Volumes/Untitled/Lessons_By_Week/Project_Rag/Final_Codes/chroma\n",
            "\n",
            "✅ Created 'retriever' variable.\n"
          ]
        }
      ],
      "source": [
        "## 6) Chunk, Embed, and Store in Vector DB\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# 1) Chunk the text\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        ")\n",
        "docs = [Document(page_content=c, metadata={\"source\": str(p.name)}) \n",
        "        for c in splitter.split_text(text)]\n",
        "print(f\"Chunks created: {len(docs)}\")\n",
        "\n",
        "# 2) Embedding function\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "# 3) Create (or re-open) the Chroma collection\n",
        "vs = Chroma(\n",
        "    collection_name=COLLECTION,\n",
        "    persist_directory=CHROMA_DIR,\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "# 4) Add docs\n",
        "vs.add_documents(docs)\n",
        "print(\"✅ Stored in Chroma at:\", Path(CHROMA_DIR).resolve())\n",
        "\n",
        "# 5) Create the retriever\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "print(\"\\n✅ Created 'retriever' variable.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "45af9258",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Multi-Query Retriever (OpenAI) is active.\n",
            "   (The system will now generate variations of your question for better search results.)\n"
          ]
        }
      ],
      "source": [
        "## 6.5) Upgrade to Multi-Query Retriever\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "import logging\n",
        "\n",
        "# Optional: Turn on logging so you can see the different questions the AI generates\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "if USE_OPENAI:\n",
        "    # This uses the LLM (gpt-4o-mini) to generate variations of the question\n",
        "    # and retrieve documents for all variations.\n",
        "    retriever = MultiQueryRetriever.from_llm(\n",
        "        retriever=vs.as_retriever(search_kwargs={\"k\": 5}),\n",
        "        llm=llm\n",
        "    )\n",
        "    print(\"✅ Multi-Query Retriever (OpenAI) is active.\")\n",
        "    print(\"   (The system will now generate variations of your question for better search results.)\")\n",
        "\n",
        "else:\n",
        "    # Fallback: Multi-query requires a strong instruction-following LLM.\n",
        "    # Smaller local models (like flan-t5) often fail the strict formatting requirements.\n",
        "    retriever = vs.as_retriever(search_kwargs={\"k\": 5})\n",
        "    print(\"ℹ️ Using standard retriever (Local Model).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "af27e405",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ RAG chain (OpenAI) is ready.\n"
          ]
        }
      ],
      "source": [
        "## 7) Build RAG Chain\n",
        "\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_docs(docs):\n",
        "    out = []\n",
        "    for i, d in enumerate(docs):\n",
        "        src = d.metadata.get(\"source\", \"\")\n",
        "        out.append(f\"[{i}] {d.page_content}\\n(source: {src})\")\n",
        "    return \"\\n\\n\".join(out)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful assistant. Answer the question only from the provided context. \"\n",
        "    \"Be friendly and answer little small talk\"\n",
        "    \"If the answer isn't present, say: 'I don't see that in the file.' \"\n",
        "    \"You may engage in friendly conversation, but never fabricate facts outside the context when answering file-based questions. \"\n",
        "    \"Give enough context so the user can understand.\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"{system}\"),\n",
        "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer succinctly:\"),\n",
        "])\n",
        "\n",
        "# Build chain\n",
        "if USE_OPENAI:\n",
        "    chain = (\n",
        "        RunnableParallel({\n",
        "            \"context\": (retriever | format_docs),\n",
        "            \"question\": RunnablePassthrough(),\n",
        "            \"system\": (lambda _: SYSTEM_PROMPT),\n",
        "        })\n",
        "        | prompt\n",
        "        | generator\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    print(\"✅ RAG chain (OpenAI) is ready.\")\n",
        "else:\n",
        "    # Emulate the same behavior for the local model in a function\n",
        "    def answer_local(question: str) -> str:\n",
        "        ctx = format_docs(retriever.get_relevant_documents(question))\n",
        "        full_prompt = (\n",
        "            f\"{SYSTEM_PROMPT}\\n\\n\"\n",
        "            f\"Question: {question}\\n\\n\"\n",
        "            f\"Context:\\n{ctx}\\n\\n\"\n",
        "            \"Answer succinctly:\"\n",
        "        )\n",
        "        return generator(full_prompt)\n",
        "\n",
        "    chain = answer_local\n",
        "    print(\"✅ RAG function (Local Transformers) is ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b23737",
      "metadata": {},
      "source": [
        "## 8) Ask Questions\n",
        "\n",
        "Run the cells below to interact with your RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "59a6db70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the 'ask' function\n",
        "def ask(question: str):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a non-empty question.\"\n",
        "    if callable(chain) and not hasattr(chain, \"invoke\"):\n",
        "        # Local HF function path\n",
        "        return chain(question)\n",
        "    # OpenAI path via LangChain\n",
        "    return chain.invoke(question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e159348e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG chatbot is ready. Ask questions based on your file.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Hello, how can I assist you today?  ', '2. What information or help are you looking for?  ', '3. Is there something specific you would like to discuss or inquire about?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Answer ---\n",
            " Hi there! How can I assist you today? If you have any questions about travel or specific topics, feel free to ask!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What is your current state or condition?  ', 'How are you feeling today?  ', 'Can you describe your well-being at the moment?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Answer ---\n",
            " I don't see that in the file. But I'm here to help! How about you?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What is the main topic or subject being discussed here?  ', 'Can you explain the purpose or focus of this content?  ', 'What information or themes does this material cover?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Answer ---\n",
            " This is about the diverse aspects of Europe, including its art, culture, festivals, and natural wonders, highlighting the interplay between human creativity and the beauty of nature. It seems to explore the richness of experiences one can find while traveling through Europe.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['What can you tell me about the Netherlands?  ', 'Can you provide information on the Netherlands?  ', 'What are some key facts about the Netherlands?']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Answer ---\n",
            " The Netherlands is known for its beautiful canals, tulip fields, and a strong cycling culture. Key attractions include Amsterdam’s Rijksmuseum and the Anne Frank House, which highlight Dutch creativity and resilience. The countryside features picturesque windmills and coastal dunes, creating an idyllic landscape.\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to chat in the console.\n",
        "# Stop with Ctrl+C (or by interrupting the kernel).\n",
        "print(\"RAG chatbot is ready. Ask questions based on your file.\")\n",
        "try:\n",
        "    while True:\n",
        "        q = input(\"\\nAsk a question (or press Enter to exit): \").strip()\n",
        "        if not q:\n",
        "            break\n",
        "        print(\"\\n--- Answer ---\\n\", ask(q))\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nChat session ended.\")\n",
        "    pass\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Rag_Env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
